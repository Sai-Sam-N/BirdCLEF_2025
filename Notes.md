## ðŸ§  Key Understanding of Dataset

### ðŸŸ¢ **Training Data**

* **train.csv** â†’ Labels & metadata for training.
* **train\_audio/** â†’ Short labeled clips (bird/amphibian/mammal/insect calls).
* Label = `primary_label` (206 possible species)

### ðŸ”µ **Unlabeled Audio**

* **train\_soundscapes/** â†’ Long-form, unlabeled 1-minute recordings (optional for semi-supervised learning)

### ðŸ”´ **Test Data**

* **test\_soundscapes/** â†’ \~700 1-min clips
* Must predict **which species are present**, **every 5s** (so: 12 segments per clip)

---

## ðŸ› ï¸ Final Deliverable

A notebook that:

* Loads each test soundscape
* Splits it into 5-second chunks
* Classifies all 206 species (multi-label)
* Outputs a `sample_submission.csv` with probability scores

---

## ðŸ§ª 8-Hour Execution Plan

### â±ï¸ HOUR 0â€“1: Dataset Setup + EDA

* âœ… Load `train.csv`
* âœ… Explore label distribution (head/tail species)
* âœ… Visualize class imbalance
* âœ… Load & plot a few `.ogg` files from `train_audio/` using `librosa`
* âœ… Optional: Map a few `primary_label`s to taxonomy with `taxonomy.csv`

### â±ï¸ HOUR 1â€“2.5: Feature Engineering

* âœ… Convert `.ogg` to **log-mel spectrograms**

  * Use `librosa.feature.melspectrogram()` or `torchaudio.transforms`
* âœ… Normalize/pad spectrograms to fixed length (e.g., 5s)
* âœ… Save them as tensors/images (to avoid repeated reprocessing)
* âœ… Define 5s windowing logic for test soundscapes

### â±ï¸ HOUR 2.5â€“4.5: Baseline Model Training

* âœ… Model: Pretrained CNN (e.g., `ResNet18`, `EfficientNetB0`) via `torchvision.models` or `Keras`
* âœ… Input: `(1, H, W)` grayscale mel-spec
* âœ… Output: 206 sigmoid activations (multi-label binary classification)
* âœ… Loss: `BCEWithLogitsLoss`
* âœ… Eval: `Average Precision`, `Label-weighted F1`, etc.

### â±ï¸ HOUR 4.5â€“6: Inference Pipeline + Submission

* âœ… Load each `.ogg` test soundscape
* âœ… Chunk into 12 Ã— 5s segments
* âœ… Apply model to each chunk
* âœ… Output predictions: `[row_id] + [206 probabilities]`
* âœ… Format as per `sample_submission.csv`

### â±ï¸ HOUR 6â€“8: Semi-Supervised Boost (Optional but Valuable)

#### Option A: Pseudo-Labeling

* Train model on labeled `train_audio`
* Predict on `train_soundscapes` with confidence threshold (e.g., > 0.9)
* Retrain with augmented data

#### Option B: Pretrained Audio Embeddings

* Use `PANNs` (Pretrained Audio Neural Networks) or `Wav2Vec2`
* Extract 1024-D features â†’ Train classifier head on top

---

## ðŸ“š Recommended Libraries

| Task                 | Library                                                      |
| -------------------- | ------------------------------------------------------------ |
| Audio loading & spec | `librosa`, `torchaudio`                                      |
| ML                   | `PyTorch`, `fastai`, `Keras`                                 |
| Pretrained models    | `torchvision`, `panns_inference`, `HuggingFace Transformers` |
| Visualization        | `matplotlib`, `seaborn`, `IPython.display.Audio`             |
| Audio augmentation   | `torch-audiomentations`, `audiomentations`                   |

---
